{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All needed import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import needed for the pre-processing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model import\n",
    "# TODO Import your model here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for the logs\n",
    "from utils.mlflow_logs import log_confusion_matrix, log_fn_and_fp, log_f1_score, log_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "RANDOM_STATE = 42\n",
    "SEED = 42\n",
    "URI = config(\"URI\")\n",
    "EXPERIMENT_ID = ...  # TODO put your experiment id here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the datasets\n",
    "DATA_PATH = ...  # TODO put the path to your dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FUNCTION = {...}  # TODO create a dictionary for all : model_name -> class   /!\\ each model must have a different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and pre-process the datas\n",
    "def get_data(frac: float = 1.0) -> Tuple:\n",
    "    \"\"\"Function used for the weather dataset\"\"\"\n",
    "\n",
    "    data = pd.read_csv(DATA_PATH).sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    target_column = ... # TODO give here the target column\n",
    "    data = data.drop([...], axis=1)  # TODO drop here the unecessary column\n",
    "    for column in [...]:  # TODO column to transform in numerical values\n",
    "        data[column] = LabelEncoder().fit_transform(data[column])\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    iforest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "    outliers = iforest.fit_predict(data)\n",
    "    clean_data = data[(outliers != -1)]\n",
    "\n",
    "    # we normalize\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    clean_array = min_max_scaler.fit_transform(clean_data)\n",
    "    clean_data = pd.DataFrame(clean_array, columns=clean_data.keys())\n",
    "\n",
    "    data_values = clean_data.drop([target_column], axis=1)\n",
    "    data_target = clean_data[target_column]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data_values, data_target, test_size=0.3, random_state=RANDOM_STATE\n",
    "    )\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_params(X_train, X_test, model_name) -> None:\n",
    "    mlflow.log_param(\"nb_features\", X_train.shape[1])\n",
    "    mlflow.log_param(\"nb_samples_train\", X_train.shape[0])\n",
    "    mlflow.log_param(\"nb_samples_test\", X_test.shape[0])\n",
    "    mlflow.log_param(\"model_name\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlflow.set_tracking_uri(URI)\n",
    "    mlflow.sklearn.autolog()  # TODO change autolog() if you are using an other library than sklearn\n",
    "    frac = ...  # TODO put here the wanted part of the total dataset to use (between 0 and 1)\n",
    "    print(\"data loading\")\n",
    "    (X_train, Y_train), (X_test, Y_test) = get_data(frac)\n",
    "    for model_name in MODEL_FUNCTION:\n",
    "        run_name = f\"Run-of-{model_name}\"  # TODO you can change the name of the form here\n",
    "        with mlflow.start_run(run_name=run_name, experiment_id=EXPERIMENT_ID):\n",
    "            model = MODEL_FUNCTION[model_name]\n",
    "            model.fit(X_train, Y_train)\n",
    "            mlflow.sklearn.log_model(model, \"model\")  # TODO change autolog() if you are using an other library than sklearn\n",
    "            log_params(X_train, X_test, model_name)\n",
    "            model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "            eval_data = X_test\n",
    "            eval_data[\"label\"] = Y_test\n",
    "            mlflow.evaluate(\n",
    "                model=model_uri,\n",
    "                data=eval_data,\n",
    "                targets=\"label\",\n",
    "                model_type=...,  # TODO complete here the type of model (\"regressor\" or \"classifier\")\n",
    "                evaluators=\"default\",\n",
    "            )\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()  # We launch it all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
