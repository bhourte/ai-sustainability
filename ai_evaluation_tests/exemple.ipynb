{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de test pour lancer de manière guidée de nombreux tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All needed import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import needed for the pre-processing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model import\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for the logs\n",
    "from utils.mlflow_logs import log_confusion_matrix, log_fn_and_fp, log_f1_score, log_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "RANDOM_STATE = 42\n",
    "SEED = 42\n",
    "URI = config(\"URI\")\n",
    "EXPERIMENT_ID = \"415539499946844786\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATA_PATH = \"datasets/solar_weather.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FUNCTION = {\n",
    "    # regression\n",
    "    \"ridge\": Ridge(),\n",
    "    \"mlp_regressor\": MLPRegressor(),\n",
    "    \"light_gmb_poisson\": HistGradientBoostingRegressor(loss=\"poisson\"),\n",
    "    \"adaboost\": AdaBoostRegressor(),\n",
    "    \"knn_regressor\": KNeighborsRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and pre-process the datas\n",
    "def get_data(frac: float = 1.0) -> Tuple:\n",
    "    \"\"\"Function used for the weather dataset\"\"\"\n",
    "\n",
    "    data = pd.read_csv(DATA_PATH).sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    target_column = \"Energy delta[Wh]\"\n",
    "    data = data.drop([\"Time\"], axis=1)\n",
    "    # No features to modify\n",
    "\n",
    "    iforest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "    outliers = iforest.fit_predict(data)\n",
    "    clean_data = data[(outliers != -1)]\n",
    "\n",
    "    # we normalize\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    clean_array = min_max_scaler.fit_transform(clean_data)\n",
    "    clean_data = pd.DataFrame(clean_array, columns=clean_data.keys())\n",
    "\n",
    "    data_values = clean_data.drop([target_column], axis=1)\n",
    "    data_target = clean_data[target_column]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data_values, data_target, test_size=0.3, random_state=RANDOM_STATE\n",
    "    )\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = get_data(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_params(X_train, X_test, model_name) -> None:\n",
    "    mlflow.log_param(\"nb_features\", X_train.shape[1])\n",
    "    mlflow.log_param(\"nb_samples_train\", X_train.shape[0])\n",
    "    mlflow.log_param(\"nb_samples_test\", X_test.shape[0])\n",
    "    mlflow.log_param(\"model_name\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlflow.set_tracking_uri(URI)\n",
    "    mlflow.sklearn.autolog()\n",
    "    frac = 0.1  # part of the total dataset to use\n",
    "    print(\"data loading\")\n",
    "    (X_train, Y_train), (X_test, Y_test) = get_data(frac)\n",
    "    for model_name in MODEL_FUNCTION:\n",
    "        run_name = f\"Run-of-{model_name}\"\n",
    "        with mlflow.start_run(run_name=run_name, experiment_id=EXPERIMENT_ID):\n",
    "            model = MODEL_FUNCTION[model_name]\n",
    "            model.fit(X_train, Y_train)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "            log_params(X_train, X_test, model_name)\n",
    "            model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "            eval_data = X_test\n",
    "            eval_data[\"label\"] = Y_test\n",
    "            mlflow.evaluate(\n",
    "                model=model_uri,\n",
    "                data=eval_data,\n",
    "                targets=\"label\",\n",
    "                model_type=\"regressor\",\n",
    "                evaluators=\"default\",\n",
    "            )\n",
    "            mlflow.end_run()\n",
    "    print(mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/24 08:43:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'e7f7326641b74398b56b90b828915937', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "2023/05/24 08:43:19 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2023/05/24 08:43:19 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\mlflow\\models\\signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2023/05/24 08:43:27 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "2023/05/24 08:43:50 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 08:43:52 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 08:43:52 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'Ridge' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 08:44:14 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 08:44:14 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor()'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 08:44:32 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 08:44:32 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: KNeighborsRegressor()'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 08:44:48 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 08:44:48 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "2023/05/24 08:44:55 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was -5.675704, while the model output was -6.449428. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 08:45:12 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 08:45:12 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor()'). Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
