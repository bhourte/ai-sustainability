{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All needed import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import needed for the pre-processing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model import\n",
    "# TODO Import your model here\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for the logs\n",
    "from utils.mlflow_logs import log_confusion_matrix, log_fn_and_fp, log_f1_score, log_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "RANDOM_STATE = 42\n",
    "SEED = 42\n",
    "URI = config(\"URI\")\n",
    "EXPERIMENT_ID = 131349728120206495  # TODO put your experiment id here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the datasets\n",
    "DATA_PATH = \"datasets/gender_classification_v7.csv\"  # TODO put the path to your dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FUNCTION = {\n",
    "    \"LightGBM with binary_crossentropy loss 1\" : HistGradientBoostingClassifier(loss=\"binary_crossentropy\", learning_rate=0.01, tol=1e-7),\n",
    "    \"LightGBM with binary_crossentropy loss 2\" : HistGradientBoostingClassifier(loss=\"binary_crossentropy\", learning_rate=0.05, tol=1e-7),\n",
    "    \"LightGBM with binary_crossentropy loss 3\" : HistGradientBoostingClassifier(loss=\"binary_crossentropy\", learning_rate=0.1, tol=1e-7),\n",
    "    \"LightGBM with binary_crossentropy loss 4\" : HistGradientBoostingClassifier(loss=\"binary_crossentropy\", learning_rate=0.01, tol=1e-5),\n",
    "    \"LightGBM with binary_crossentropy loss 5\" : HistGradientBoostingClassifier(loss=\"binary_crossentropy\", learning_rate=0.05, tol=1e-5),\n",
    "    \"LightGBM with binary_crossentropy loss 6\" : HistGradientBoostingClassifier(loss=\"binary_crossentropy\", learning_rate=0.1, tol=1e-5),\n",
    "    \"LightGBM with log_loss loss 1\" : HistGradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.01, tol=1e-7),\n",
    "    \"LightGBM with log_loss loss 2\" : HistGradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.05, tol=1e-7),\n",
    "    \"LightGBM with log_loss loss 3\" : HistGradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.1, tol=1e-7),\n",
    "    \"LightGBM with log_loss loss 4\" : HistGradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.01, tol=1e-5),\n",
    "    \"LightGBM with log_loss loss 5\" : HistGradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.05, tol=1e-5),\n",
    "    \"LightGBM with log_loss loss 6\" : HistGradientBoostingClassifier(loss=\"log_loss\", learning_rate=0.1, tol=1e-5),\n",
    "    \"Multi layer perceptron 1\" : MLPClassifier(hidden_layer_sizes=100, activation=\"identity\"),\n",
    "    \"Multi layer perceptron 2\" : MLPClassifier(hidden_layer_sizes=100, activation=\"logistic\"),\n",
    "    \"Multi layer perceptron 3\" : MLPClassifier(hidden_layer_sizes=100, activation=\"tanh\"),\n",
    "    \"Multi layer perceptron 4\" : MLPClassifier(hidden_layer_sizes=100, activation=\"relu\"),\n",
    "    \"Multi layer perceptron 5\" : MLPClassifier(hidden_layer_sizes=150, activation=\"identity\"),\n",
    "    \"Multi layer perceptron 6\" : MLPClassifier(hidden_layer_sizes=150, activation=\"logistic\"),\n",
    "    \"Multi layer perceptron 7\" : MLPClassifier(hidden_layer_sizes=150, activation=\"tanh\"),\n",
    "    \"Multi layer perceptron 8\" : MLPClassifier(hidden_layer_sizes=150, activation=\"relu\"),\n",
    "    \"Random Forest 1\" : RandomForestClassifier(n_estimators=50, criterion=\"gini\"),\n",
    "    \"Random Forest 2\" : RandomForestClassifier(n_estimators=50, criterion=\"entropy\"),\n",
    "    \"Random Forest 3\" : RandomForestClassifier(n_estimators=50, criterion=\"log_loss\"),\n",
    "    \"Random Forest 4\" : RandomForestClassifier(n_estimators=100, criterion=\"gini\"),\n",
    "    \"Random Forest 5\" : RandomForestClassifier(n_estimators=100, criterion=\"entropy\"),\n",
    "    \"Random Forest 6\" : RandomForestClassifier(n_estimators=100, criterion=\"log_loss\"),\n",
    "    \"SVC with rbf 1\" : SVC(kernel=\"linear\", C=1, gamma=\"scale\", random_state=RANDOM_STATE),\n",
    "    \"SVC with rbf 2\" : SVC(kernel=\"linear\", C=0.5, gamma=\"scale\", random_state=RANDOM_STATE),\n",
    "    \"SVC with rbf 3\" : SVC(kernel=\"linear\", C=10, gamma=\"scale\", random_state=RANDOM_STATE),\n",
    "    \"SVC with rbf 4\" : SVC(kernel=\"linear\", C=1, gamma=\"auto\", random_state=RANDOM_STATE),\n",
    "    \"SVC with rbf 5\" : SVC(kernel=\"linear\", C=0.5, gamma=\"auto\", random_state=RANDOM_STATE),\n",
    "    \"SVC with rbf 6\" : SVC(kernel=\"linear\", C=10, gamma=\"auto\", random_state=RANDOM_STATE),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and pre-process the datas\n",
    "def get_data(frac: float = 1.0) -> Tuple:\n",
    "    \"\"\"Function used for the weather dataset\"\"\"\n",
    "\n",
    "    data = pd.read_csv(DATA_PATH).sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    target_column = \"gender\" # TODO give here the target column\n",
    "    data = data.drop([], axis=1)  # TODO drop here the unecessary column\n",
    "    for column in [\"gender\"]:  # TODO column to transform in numerical values\n",
    "        data[column] = LabelEncoder().fit_transform(data[column])\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    iforest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "    outliers = iforest.fit_predict(data)\n",
    "    clean_data = data[(outliers != -1)]\n",
    "\n",
    "    # we normalize\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    clean_array = min_max_scaler.fit_transform(clean_data)\n",
    "    clean_data = pd.DataFrame(clean_array, columns=clean_data.keys())\n",
    "\n",
    "    data_values = clean_data.drop([target_column], axis=1)\n",
    "    data_target = clean_data[target_column]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data_values, data_target, test_size=0.3, random_state=RANDOM_STATE\n",
    "    )\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_params(X_train, X_test, model_name) -> None:\n",
    "    mlflow.log_param(\"nb_features\", X_train.shape[1])\n",
    "    mlflow.log_param(\"nb_samples_train\", X_train.shape[0])\n",
    "    mlflow.log_param(\"nb_samples_test\", X_test.shape[0])\n",
    "    mlflow.log_param(\"model_name\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlflow.set_tracking_uri(URI)\n",
    "    mlflow.sklearn.autolog()  # TODO change autolog() if you are using an other library than sklearn\n",
    "    frac = 1  # TODO put here the wanted part of the total dataset to use (between 0 and 1)\n",
    "    print(\"data loading\")\n",
    "    (X_train, Y_train), (X_test, Y_test) = get_data(frac)\n",
    "    for model_name in MODEL_FUNCTION:\n",
    "        run_name = f\"{model_name}\"  # TODO you can change the name of the form here\n",
    "        with mlflow.start_run(run_name=run_name, experiment_id=EXPERIMENT_ID):\n",
    "            model = MODEL_FUNCTION[model_name]\n",
    "            model.fit(X_train, Y_train)\n",
    "            mlflow.sklearn.log_model(model, \"model\")  # TODO change autolog() if you are using an other library than sklearn\n",
    "            log_params(X_train, X_test, model_name)\n",
    "            model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "            eval_data = X_test\n",
    "            eval_data[\"label\"] = Y_test\n",
    "            mlflow.evaluate(\n",
    "                model=model_uri,\n",
    "                data=eval_data,\n",
    "                targets=\"label\",\n",
    "                model_type=\"classifier\",  # TODO complete here the type of model (\"regressor\" or \"classifier\")\n",
    "                evaluators=\"default\",\n",
    "            )\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/01 09:36:51 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd4c6c34903ed4a60936cb07ade1976f9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "2023/06/01 09:36:52 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2023/06/01 09:36:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\mlflow\\models\\signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2023/06/01 09:37:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1823: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "2023/06/01 09:37:25 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:37:25 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:37:31 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:37:36 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "2023/06/01 09:37:58 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:37:58 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:38:00 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:38:08 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "2023/06/01 09:38:25 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:38:25 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:38:27 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:38:34 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "2023/06/01 09:38:52 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:38:52 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:38:54 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:39:00 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "2023/06/01 09:39:17 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:39:17 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:39:19 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:39:27 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
      "2023/06/01 09:39:48 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:39:48 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:39:50 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 99%|===================| 1336/1351 [00:11<00:00]       Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:40:01 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:40:21 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:40:21 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:40:23 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:40:32 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:40:54 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:40:55 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:40:57 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:41:06 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:41:32 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:41:32 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:41:35 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:41:44 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:42:04 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:42:04 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:42:06 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:42:12 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:42:29 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:42:29 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:42:31 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:42:39 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:42:59 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:43:00 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:43:06 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 99%|===================| 1333/1351 [00:11<00:00]       Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:43:17 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:43:38 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:43:38 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:43:39 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(activation='identity', hidden_layer_sizes=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:43:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:43:57 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:43:59 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(activation='logistic', hidden_layer_sizes=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:44:15 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:44:15 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:44:16 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(activation='tanh', hidden_layer_sizes=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:44:32 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:44:32 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:44:34 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(hidden_layer_sizes=100)'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:44:49 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:44:49 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:44:50 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(activation='identity', hidden_layer_sizes=150)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:45:07 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:45:07 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:45:08 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(activation='logistic', hidden_layer_sizes=150)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:45:24 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:45:24 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:45:26 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(activation='tanh', hidden_layer_sizes=150)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:45:43 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:45:43 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:45:45 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(hidden_layer_sizes=150)'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:46:03 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:46:03 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:46:05 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 93%|=================== | 2526/2702 [00:13<00:00]       2023/06/01 09:46:18 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.779600, while the model output was 0.660000. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:46:35 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:46:35 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:46:37 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 97%|=================== | 2610/2702 [00:11<00:00]       2023/06/01 09:46:48 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.680000, while the model output was 0.600000. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:47:03 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:47:03 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:47:05 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 99%|===================| 2679/2702 [00:12<00:00]        2023/06/01 09:47:17 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.839400, while the model output was 0.760000. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:47:32 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:47:32 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:47:33 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 97%|=================== | 2617/2702 [00:25<00:00]       2023/06/01 09:47:58 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.499600, while the model output was 0.400000. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:48:14 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:48:14 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:48:16 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 97%|=================== | 2609/2702 [00:22<00:00]       2023/06/01 09:48:38 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.649800, while the model output was 0.590000. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:48:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:48:56 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:48:58 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      " 99%|===================| 2681/2702 [00:23<00:00]        2023/06/01 09:49:21 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: ExplainerError('Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.709900, while the model output was 0.650000. If this difference is acceptable you can set check_additivity=False to disable this check.'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:49:35 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:49:35 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:49:36 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:49:36 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'SVC' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:49:57 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:49:57 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:49:58 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:49:58 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'SVC' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:50:28 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:50:28 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:50:29 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:50:29 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'SVC' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:50:46 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:50:46 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:50:47 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:50:47 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'SVC' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:51:07 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:51:07 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:51:08 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:51:08 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'SVC' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/06/01 09:51:28 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/06/01 09:51:28 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1.0, negative label is 0.0.\n",
      "2023/06/01 09:51:28 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/06/01 09:51:29 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'SVC' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "main()  # We launch it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/01 10:18:51 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '3a5c5b22ab5f48bcbe2c20add6e17e32', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "2023/06/01 10:18:52 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2023/06/01 10:18:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\mlflow\\models\\signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. ser.count(level=1) should use ser.groupby(level=1).count().\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Series.count level is only valid with a MultiIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14784\\2304944516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self, level)\u001b[0m\n\u001b[0;32m   1974\u001b[0m             )\n\u001b[0;32m   1975\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1976\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Series.count level is only valid with a MultiIndex\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1978\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Series.count level is only valid with a MultiIndex"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = get_data(1)\n",
    "print(Y_test.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n",
      "662\n"
     ]
    }
   ],
   "source": [
    "nb_0 = 0\n",
    "nb_1 = 0\n",
    "for i in Y_test.to_numpy():\n",
    "    if i == 1:\n",
    "        nb_1 += 1\n",
    "    elif i == 0:\n",
    "        nb_0 += 1\n",
    "    else:\n",
    "        print(\"Non-binaire\")\n",
    "print(nb_0)\n",
    "print(nb_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
